Problem Statement 1- Using Apache Spark and Python, Read and Preprocess rows to insure further obtimal structure and Performance
					 for Further Preprocessing.
Problem Statement 2- Using Apache Spark and Python, Read and processed data set From step1 and ;
						1-Extract only Recipes that have Beef as one of the ingredients 
						2-Calculate Average cooking time duration per Difficulty level 

# Installation
	PySpark Version-"3.0.0"(set the path in enviroments)
        python version 3.8
        jdk 8(set the path in enviroments)
	Jupyter NoteBook
#Libraries
	Pandas
	Numpy
	matplotlib
#Process
	-Extrated Data from PySpark and Save it to local System
	 as Data1.csv
	-Load data in Jupyter NoteBook
	-Perform EDA(Exploratory Data Analysis)
	-Extract the Beef cantain in Recipes
	-calculated Total Cooking Time
	-Calculated Average Cooking Time
	-Labled the Recipes according to Difficulty Level
        -output in report.csv
	
		
